{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77196bc8-ac98-4439-bb93-a24cf7ff5128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# super ttt. 4row4col5diag. board 5x(4x4). 1/2 prob accept, 1/16 around(forfeit if occupied or outside). tfagent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03f8e15f-108c-4ad6-a93f-7ab972054fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from tensorflow.keras.initializers import RandomNormal, GlorotNormal, Zeros\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5692279b-cf80-4cdb-b5c9-a366d142596d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# tictactoe env\n",
    "class SuperTicTacToeEnv:\n",
    "    \"\"\"\n",
    "    Environment for the Super Tic-Tac-Toe game.\n",
    "\n",
    "    The game is played on 5 boards, each of which is a 4x4 grid.\n",
    "    Two players take turns placing their marks (1 or -1) on the boards.\n",
    "    The first player to get 4 in a row, column, or diagonal wins.\n",
    "    If all squares are filled and no player has won, the game is a draw.\n",
    "\n",
    "    Attributes:\n",
    "        _state (np.ndarray): The current state of the game.\n",
    "        _current_player (int): The current player (1 or -1).\n",
    "        _num_actions (int): The number of possible actions.\n",
    "        _episode_ended (bool): Whether the episode has ended.\n",
    "        nearbys (list): List of nearby squares.\n",
    "        board_layout (dict): Layout of the boards.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the SuperTicTacToeEnv.\n",
    "        \"\"\"\n",
    "        self._state = np.zeros((5, 4, 4), dtype=np.int32)  # 5 boards, of which each 4x4\n",
    "        self._current_player = 1  # two players, represented by 1 and -1\n",
    "        self._num_actions = 5 * 4 * 4\n",
    "        self._episode_ended = False\n",
    "        self.nearbys = [(-1, -1), (-1, 0), (-1, 1), (0, -1), (0, 1), (1, -1), (1, 0), (1, 1)]\n",
    "        self.board_layout = {  # not used, just for understand\n",
    "            0: (-1, 0),  # up\n",
    "            1: (0, -1),  # left\n",
    "            2: (0, 0),   # center\n",
    "            3: (0, 1),   # right\n",
    "            4: (1, 0)    # down\n",
    "        }\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the environment to the initial state.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The initial state of the game.\n",
    "        \"\"\"\n",
    "        self._state = np.zeros((5, 4, 4), dtype=np.int32)\n",
    "        self._current_player = 1\n",
    "        self._episode_ended = False\n",
    "        return self._state\n",
    "\n",
    "    def step(self, action, verbose=False):\n",
    "        \"\"\"\n",
    "        Takes a step in the environment.\n",
    "\n",
    "        Args:\n",
    "            action (int): The action to take.\n",
    "            verbose (bool, optional): Whether to print debugging information. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            tuple: The next state, reward, and whether the episode has ended.\n",
    "        \"\"\"\n",
    "        if self._episode_ended:\n",
    "            return self.reset(), 0, self._episode_ended\n",
    "\n",
    "        # parse action to (board, row, col)\n",
    "        board = action // (4 * 4)\n",
    "        row = (action % (4 * 4)) // 4\n",
    "        col = (action % (4 * 4)) % 4\n",
    "        if verbose:\n",
    "            print(\"player {} action {}, board {}, row {}, col {}\".format(self._current_player, action, board, row, col))\n",
    "\n",
    "        # selection occupied, end with negative reward\n",
    "        occupied_penalty = 0\n",
    "        if self._state[board, row, col] != 0:\n",
    "            occupied_penalty = - 1.0\n",
    "            if verbose:\n",
    "                print(\"action {} occupied already, reward {}\".format(action, occupied_penalty))\n",
    "            # self._episode_ended = True  # do not end episode. state unchanged\n",
    "            # return self._state, reward, self._episode_ended\n",
    "        elif np.random.rand() < 0.5:  # action accepted\n",
    "            self._state[board, row, col] = self._current_player\n",
    "            if verbose:\n",
    "                print(\"action {} accepted directly\".format(action))\n",
    "        else:  # randomly select a nearby square\n",
    "            drow, dcol = random.choice(self.nearbys)\n",
    "            nrow, ncol = row + drow, col + dcol\n",
    "\n",
    "            # move across boards\n",
    "            if board == 0 and 0 <= ncol <= 3 and nrow > 3:  # up board goes to center board\n",
    "                nboard = 2\n",
    "                nrow -= 4\n",
    "            elif board == 1 and 0 <= nrow <= 3 and ncol > 3:  # left board goes to center board\n",
    "                nboard = 2\n",
    "                ncol -= 4\n",
    "            elif board == 3 and 0 <= nrow <= 3 and ncol < 0:  # right board goes to center board\n",
    "                nboard = 2\n",
    "                ncol += 4\n",
    "            elif board == 4 and 0 <= ncol <= 3 and nrow < 0:  # down board goes to center board\n",
    "                nboard = 2\n",
    "                nrow += 4\n",
    "            elif board == 2:  # center board goes to nearby board\n",
    "                if 0 <= ncol <= 3 and nrow < 0:  # up\n",
    "                    nboard = 0\n",
    "                    nrow += 4\n",
    "                elif 0 <= ncol <= 3 and nrow > 3:  # down\n",
    "                    nboard = 4\n",
    "                    nrow -= 4\n",
    "                elif 0 <= nrow <= 3 and ncol < 0:  # left\n",
    "                    nboard = 1\n",
    "                    ncol += 4\n",
    "                elif 0 <= nrow <= 3 and ncol > 3:  # right\n",
    "                    nboard = 3\n",
    "                    ncol -= 4\n",
    "                else:\n",
    "                    nboard = board\n",
    "            else:  # move\n",
    "                nboard = board\n",
    "            if verbose:\n",
    "                print(\"action {} shift from (b{}, r{} c{}) to (b{}, r{}, c{}), d=({}, {})\".format(action, board, row, col, nboard, nrow, ncol, drow, dcol))\n",
    "\n",
    "            if (0 <= nrow <= 3 and 0 <= ncol <= 3) and (self._state[nboard, nrow, ncol] == 0):  # empty legal square\n",
    "                self._state[nboard, nrow, ncol] = self._current_player\n",
    "                if verbose:\n",
    "                    print(\"action {} accepted\".format(action))\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(\"action {} denied\".format(action))\n",
    "                pass\n",
    "\n",
    "        if self._check_win(self._current_player):\n",
    "            reward = 1.0\n",
    "            self._episode_ended = True\n",
    "        elif np.all(self._state != 0):\n",
    "            reward = 0\n",
    "            self._episode_ended = True\n",
    "        else:\n",
    "            self._current_player = -self._current_player\n",
    "            reward = occupied_penalty\n",
    "        if verbose:\n",
    "            print(\"reward {}, episode_ended {}\".format(reward, self._episode_ended))\n",
    "            print(\"boards:\")\n",
    "            self._draw_boards()\n",
    "        return self._state, reward, self._episode_ended\n",
    "\n",
    "    def _full_boards(self):\n",
    "        \"\"\"\n",
    "        Pads the boards to create a full 12x12 grid.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: The padded boards.\n",
    "        \"\"\"\n",
    "        # (5, 4, 4) boards padded to (9, 4, 4) boards, final shape (3x4, 3x4).\n",
    "        padded_states = np.concatenate(\n",
    "            [np.concatenate([np.zeros([4, 4]), self._state[0], np.zeros([4, 4])], axis=-1),\n",
    "             np.concatenate([self._state[1], self._state[2], self._state[3]], axis=-1),\n",
    "             np.concatenate([np.zeros([4, 4]), self._state[4], np.zeros([4, 4])], axis=-1),\n",
    "            ], axis=0).astype(np.int64)\n",
    "        return padded_states\n",
    "\n",
    "    def _draw_boards(self):\n",
    "        \"\"\"\n",
    "        Prints the current state of the game boards in a human-readable format.\n",
    "        \"\"\"\n",
    "        boards = self._full_boards()\n",
    "        for row_idx in range(12):\n",
    "            curr_row = \"\"\n",
    "            for col_idx in range(12):\n",
    "                if (row_idx <= 3 or row_idx >= 8) and (col_idx <= 3 or col_idx >= 8):\n",
    "                    curr_row += \"   \"\n",
    "                else:\n",
    "                    curr_row += \"{:3d}\".format(boards[row_idx][col_idx])\n",
    "            print(curr_row)\n",
    "\n",
    "    def _check_win(self, player):\n",
    "        \"\"\"\n",
    "        Checks if the specified player has won the game.\n",
    "\n",
    "        Args:\n",
    "            player (int): The player to check (1 or -1).\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the player has won, False otherwise.\n",
    "        \"\"\"\n",
    "        padded_states = self._full_boards()\n",
    "        rows, cols = 12, 12\n",
    "        # check 4 in a row\n",
    "        for row in padded_states:\n",
    "            for i in range(cols - 3):\n",
    "                if all(row[j] == player for j in range(i, i + 4)):\n",
    "                    return True\n",
    "        # check 4 in a col\n",
    "        for colidx in range(cols):\n",
    "            col = [padded_states[rowidx][colidx] for rowidx in range(rows)]\n",
    "            for i in range(rows - 3):\n",
    "                if all(col[j] == player for j in range(i, i + 4)):\n",
    "                    return True\n",
    "\n",
    "        # check 5 in a diag\n",
    "        for i in range(rows - 4):\n",
    "            for j in range(cols - 4):\n",
    "                diag1 = [padded_states[i + k][j + k] for k in range(5)]  # upleft to downright\n",
    "                if all(cell == player for cell in diag1):\n",
    "                    return True\n",
    "                diag2 = [padded_states[i + k][j + 4 - k] for k in range(5)]  # upright to downleft\n",
    "                if all(cell == player for cell in diag2):\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "env = SuperTicTacToeEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ddebbe0-505e-4d63-91bf-4ade9da0bc81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player 1 action 10, board 0, row 2, col 2\n",
      "action 10 shift from (b0, r2 c2) to (b0, r3, c2), d=(1, 0)\n",
      "action 10 accepted\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  1  0            \n",
      "  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "player -1 action 22, board 1, row 1, col 2\n",
      "action 22 accepted directly\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  1  0            \n",
      "  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0 -1  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "player 1 action 55, board 3, row 1, col 3\n",
      "action 55 shift from (b3, r1 c3) to (b3, r1, c4), d=(0, 1)\n",
      "action 55 denied\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  1  0            \n",
      "  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0 -1  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "player -1 action 22, board 1, row 1, col 2\n",
      "action 22 occupied already, reward -1.0\n",
      "reward -1.0, episode_ended False\n",
      "boards:\n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  1  0            \n",
      "  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0 -1  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "player 1 action 56, board 3, row 2, col 0\n",
      "action 56 shift from (b3, r2 c0) to (b3, r3, c0), d=(1, 0)\n",
      "action 56 accepted\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  1  0            \n",
      "  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0 -1  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  1  0  0  0\n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "player -1 action 73, board 4, row 2, col 1\n",
      "action 73 shift from (b4, r2 c1) to (b4, r3, c2), d=(1, 1)\n",
      "action 73 accepted\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  1  0            \n",
      "  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0 -1  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  1  0  0  0\n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0 -1  0            \n",
      "player 1 action 73, board 4, row 2, col 1\n",
      "action 73 shift from (b4, r2 c1) to (b4, r3, c2), d=(1, 1)\n",
      "action 73 denied\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  1  0            \n",
      "  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0 -1  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  1  0  0  0\n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0 -1  0            \n",
      "player -1 action 54, board 3, row 1, col 2\n",
      "action 54 accepted directly\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  1  0            \n",
      "  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0 -1  0  0  0  0  0  0  0 -1  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  1  0  0  0\n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0 -1  0            \n",
      "player 1 action 48, board 3, row 0, col 0\n",
      "action 48 accepted directly\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  1  0            \n",
      "  0  0  0  0  0  0  0  0  1  0  0  0\n",
      "  0  0 -1  0  0  0  0  0  0  0 -1  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  1  0  0  0\n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0 -1  0            \n",
      "player -1 action 12, board 0, row 3, col 0\n",
      "action 12 shift from (b0, r3 c0) to (b2, r0, c1), d=(1, 1)\n",
      "action 12 accepted\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  1  0            \n",
      "  0  0  0  0  0 -1  0  0  1  0  0  0\n",
      "  0  0 -1  0  0  0  0  0  0  0 -1  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  1  0  0  0\n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0 -1  0            \n",
      "player 1 action 15, board 0, row 3, col 3\n",
      "action 15 accepted directly\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  1  1            \n",
      "  0  0  0  0  0 -1  0  0  1  0  0  0\n",
      "  0  0 -1  0  0  0  0  0  0  0 -1  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  1  0  0  0\n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0 -1  0            \n",
      "player -1 action 22, board 1, row 1, col 2\n",
      "action 22 occupied already, reward -1.0\n",
      "reward -1.0, episode_ended False\n",
      "boards:\n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  1  1            \n",
      "  0  0  0  0  0 -1  0  0  1  0  0  0\n",
      "  0  0 -1  0  0  0  0  0  0  0 -1  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  1  0  0  0\n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0 -1  0            \n",
      "player 1 action 73, board 4, row 2, col 1\n",
      "action 73 accepted directly\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  1  1            \n",
      "  0  0  0  0  0 -1  0  0  1  0  0  0\n",
      "  0  0 -1  0  0  0  0  0  0  0 -1  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  1  0  0  0\n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  1  0  0            \n",
      "              0  0 -1  0            \n",
      "player -1 action 20, board 1, row 1, col 0\n",
      "action 20 shift from (b1, r1 c0) to (b1, r2, c-1), d=(1, -1)\n",
      "action 20 denied\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  1  1            \n",
      "  0  0  0  0  0 -1  0  0  1  0  0  0\n",
      "  0  0 -1  0  0  0  0  0  0  0 -1  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  1  0  0  0\n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  1  0  0            \n",
      "              0  0 -1  0            \n",
      "player 1 action 67, board 4, row 0, col 3\n",
      "action 67 accepted directly\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  1  1            \n",
      "  0  0  0  0  0 -1  0  0  1  0  0  0\n",
      "  0  0 -1  0  0  0  0  0  0  0 -1  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  1  0  0  0\n",
      "              0  0  0  1            \n",
      "              0  0  0  0            \n",
      "              0  1  0  0            \n",
      "              0  0 -1  0            \n",
      "player -1 action 64, board 4, row 0, col 0\n",
      "action 64 shift from (b4, r0 c0) to (b4, r0, c1), d=(0, 1)\n",
      "action 64 accepted\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  1  1            \n",
      "  0  0  0  0  0 -1  0  0  1  0  0  0\n",
      "  0  0 -1  0  0  0  0  0  0  0 -1  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  1  0  0  0\n",
      "              0 -1  0  1            \n",
      "              0  0  0  0            \n",
      "              0  1  0  0            \n",
      "              0  0 -1  0            \n",
      "player 1 action 74, board 4, row 2, col 2\n",
      "action 74 accepted directly\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  1  1            \n",
      "  0  0  0  0  0 -1  0  0  1  0  0  0\n",
      "  0  0 -1  0  0  0  0  0  0  0 -1  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  1  0  0  0\n",
      "              0 -1  0  1            \n",
      "              0  0  0  0            \n",
      "              0  1  1  0            \n",
      "              0  0 -1  0            \n",
      "player -1 action 37, board 2, row 1, col 1\n",
      "action 37 shift from (b2, r1 c1) to (b2, r1, c0), d=(0, -1)\n",
      "action 37 accepted\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  1  1            \n",
      "  0  0  0  0  0 -1  0  0  1  0  0  0\n",
      "  0  0 -1  0 -1  0  0  0  0  0 -1  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  1  0  0  0\n",
      "              0 -1  0  1            \n",
      "              0  0  0  0            \n",
      "              0  1  1  0            \n",
      "              0  0 -1  0            \n",
      "player 1 action 27, board 1, row 2, col 3\n",
      "action 27 accepted directly\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  1  1            \n",
      "  0  0  0  0  0 -1  0  0  1  0  0  0\n",
      "  0  0 -1  0 -1  0  0  0  0  0 -1  0\n",
      "  0  0  0  1  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  1  0  0  0\n",
      "              0 -1  0  1            \n",
      "              0  0  0  0            \n",
      "              0  1  1  0            \n",
      "              0  0 -1  0            \n",
      "player -1 action 1, board 0, row 0, col 1\n",
      "action 1 accepted directly\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0 -1  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  1  1            \n",
      "  0  0  0  0  0 -1  0  0  1  0  0  0\n",
      "  0  0 -1  0 -1  0  0  0  0  0 -1  0\n",
      "  0  0  0  1  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  1  0  0  0\n",
      "              0 -1  0  1            \n",
      "              0  0  0  0            \n",
      "              0  1  1  0            \n",
      "              0  0 -1  0            \n"
     ]
    }
   ],
   "source": [
    "# test environment implementation of step()\n",
    "env.reset()\n",
    "for i in range(20):\n",
    "    x = random.randint(0, 5 * 4 * 4 -1)\n",
    "    env.step(x, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93d3c915-b514-490a-a507-2e4adc8c5425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False False\n",
      "False True\n",
      "False False\n",
      "True False\n",
      "False False\n",
      "True False\n",
      "False False\n",
      "False True\n",
      "False True\n",
      "False False\n",
      "False True\n",
      "True False\n"
     ]
    }
   ],
   "source": [
    "# test environment implementation of check_win()\n",
    "# check init\n",
    "env.reset()\n",
    "print(env._check_win(-1), env._check_win(1))\n",
    "\n",
    "# check row in one board\n",
    "env.reset()\n",
    "env._state[4][0][0] = 1\n",
    "env._state[4][0][1] = 1\n",
    "env._state[4][0][2] = 1\n",
    "env._state[4][0][3] = 1\n",
    "print(env._check_win(-1), env._check_win(1))\n",
    "env._state[4][0][3] = -1\n",
    "env._state[4][1][3] = 1\n",
    "print(env._check_win(-1), env._check_win(1))\n",
    "\n",
    "# check row in two boards\n",
    "env.reset()\n",
    "env._state[2][2][2] = -1\n",
    "env._state[2][2][3] = -1\n",
    "env._state[3][2][0] = -1\n",
    "env._state[3][2][1] = -1\n",
    "print(env._check_win(-1), env._check_win(1))\n",
    "env._state[3][2][1] = 1\n",
    "env._state[3][2][2] = -1\n",
    "print(env._check_win(-1), env._check_win(1))\n",
    "\n",
    "# check col in one board\n",
    "env.reset()\n",
    "env._state[0][0][3] = -1\n",
    "env._state[0][1][3] = -1\n",
    "env._state[0][2][3] = -1\n",
    "env._state[0][3][3] = -1\n",
    "print(env._check_win(-1), env._check_win(1))\n",
    "env._state[0][3][3] = 1\n",
    "env._state[0][3][2] = -1\n",
    "print(env._check_win(-1), env._check_win(1))\n",
    "\n",
    "# check col in two boards\n",
    "env.reset()\n",
    "env._state[0][3][1] = 1\n",
    "env._state[2][0][1] = 1\n",
    "env._state[2][1][1] = 1\n",
    "env._state[2][2][1] = 1\n",
    "print(env._check_win(-1), env._check_win(1))\n",
    "env._state[2][2][1] = -1\n",
    "env._state[0][2][1] = 1\n",
    "print(env._check_win(-1), env._check_win(1))\n",
    "env._state[2][1][1] = -1\n",
    "print(env._check_win(-1), env._check_win(1))\n",
    "\n",
    "# check diag, across boards\n",
    "env.reset()\n",
    "env._state[0][2][2] = 1\n",
    "env._state[0][3][1] = 1\n",
    "env._state[2][0][0] = 1\n",
    "env._state[1][1][3] = 1\n",
    "env._state[1][2][2] = 1\n",
    "env._state[0][2][0] = -1\n",
    "env._state[2][0][2] = -1\n",
    "env._state[2][1][3] = -1\n",
    "env._state[3][2][0] = -1\n",
    "# env._draw_boards()\n",
    "print(env._check_win(-1), env._check_win(1))\n",
    "env._state[0][3][1] = -1\n",
    "# env._draw_boards()\n",
    "print(env._check_win(-1), env._check_win(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42603236-01f3-414b-9e2e-cb94b54dff6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Deep Q-Network (DQN) model for reinforcement learning.\n",
    "\n",
    "    The model predicts Q-values for each possible action given the current state.\n",
    "\n",
    "    Attributes:\n",
    "        num_actions (int): The number of possible actions.\n",
    "        flatten (tf.keras.layers.Layer): Layer to flatten the input.\n",
    "        dense1, dense2, dense3, dense4 (tf.keras.layers.Layer): Dense layers for the network.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_actions):\n",
    "        \"\"\"\n",
    "        Initializes the DQN model with the specified number of actions.\n",
    "\n",
    "        Args:\n",
    "            num_actions (int): The number of possible actions.\n",
    "        \"\"\"\n",
    "        super(DQN, self).__init__()\n",
    "        self.num_actions = num_actions\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense1 = tf.keras.layers.Dense(256, activation='relu', kernel_initializer=GlorotNormal(), bias_initializer=Zeros())\n",
    "        self.dense2 = tf.keras.layers.Dense(128, activation='relu', kernel_initializer=GlorotNormal(), bias_initializer=Zeros())\n",
    "        self.dense3 = tf.keras.layers.Dense(128, activation='relu', kernel_initializer=GlorotNormal(), bias_initializer=Zeros())\n",
    "        self.dense4 = tf.keras.layers.Dense(num_actions, activation='linear', kernel_initializer=GlorotNormal(), bias_initializer=Zeros())\n",
    "\n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        Performs a forward pass through the network.\n",
    "\n",
    "        Args:\n",
    "            x (tf.Tensor): The input tensor.\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: The output Q-values for each action.\n",
    "        \"\"\"\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dense3(x)\n",
    "        return self.dense4(x)\n",
    "\n",
    "    def epsilon_greedy(self, state, curr_player, epsilon=0.1):\n",
    "        \"\"\"\n",
    "        Selects an action using the epsilon-greedy strategy.\n",
    "\n",
    "        Args:\n",
    "            state (np.ndarray): The current state of the environment.\n",
    "            curr_player (int): The current player (1 or -1).\n",
    "            epsilon (float): The probability of selecting a random action.\n",
    "\n",
    "        Returns:\n",
    "            int: The selected action.\n",
    "        \"\"\"\n",
    "        curr_player = env._current_player\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = np.random.choice(self.num_actions)\n",
    "        else:\n",
    "            state_ = np.concatenate([state.flatten().reshape([1, -1]), np.array(curr_player).reshape(1, 1)], axis=1)\n",
    "            q_values = dqn(state_)\n",
    "            # penalty for occupied actions\n",
    "            penalty = np.where(state.reshape([1, -1]) == 0, 0, -1e6)\n",
    "            q_values += penalty\n",
    "            # action = np.argmax(q_values.numpy()[0]). random select when there is a tie\n",
    "            max_qval = np.max(q_values)\n",
    "            max_indices = np.where(q_values == max_qval)[1]  # [1] takes the indices\n",
    "            action = np.random.choice(max_indices)\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda93f72-2e84-470f-8327-734a648ad6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    Replay buffer for storing and sampling experience tuples.\n",
    "\n",
    "    Attributes:\n",
    "        capacity (int): The maximum number of experiences to store.\n",
    "        buffer (list): The list of stored experiences.\n",
    "        position (int): The current position for overwriting old experiences.\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        \"\"\"\n",
    "        Initializes the replay buffer with the specified capacity.\n",
    "\n",
    "        Args:\n",
    "            capacity (int): The maximum number of experiences to store.\n",
    "        \"\"\"\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done, curr_player):\n",
    "        \"\"\"\n",
    "        Adds a new experience to the buffer.\n",
    "\n",
    "        Args:\n",
    "            state (np.ndarray): The current state.\n",
    "            action (int): The action taken.\n",
    "            reward (float): The reward received.\n",
    "            next_state (np.ndarray): The next state.\n",
    "            done (bool): Whether the episode has ended.\n",
    "            curr_player (int): The current player.\n",
    "        \"\"\"\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done, curr_player)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Samples a batch of experiences from the buffer.\n",
    "\n",
    "        Args:\n",
    "            batch_size (int): The number of experiences to sample.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing arrays of states, actions, rewards, next states, dones, and current players.\n",
    "        \"\"\"\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done, curr_player = zip(*batch)\n",
    "        return np.array(state), np.array(action), np.array(reward), np.array(next_state), np.array(done), np.array(curr_player)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the number of experiences currently stored in the buffer.\n",
    "\n",
    "        Returns:\n",
    "            int: The number of stored experiences.\n",
    "        \"\"\"\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e358f1-0451-4a5a-b65e-521bd01d2ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dqn(env, dqn, num_games=10, max_steps=1000):\n",
    "    \"\"\"\n",
    "    Tests the DQN model by playing a specified number of games.\n",
    "\n",
    "    Args:\n",
    "        env (SuperTicTacToeEnv): The game environment.\n",
    "        dqn (DQN): The trained DQN model.\n",
    "        num_games (int): The number of games to play.\n",
    "        max_steps (int): The maximum number of steps per game.\n",
    "    \"\"\"\n",
    "    for _ in range(num_games):\n",
    "        state = env.reset()\n",
    "        state = np.array([state])\n",
    "        done = False\n",
    "        num_steps = 0\n",
    "        while not done:\n",
    "            curr_player = env._current_player\n",
    "            action = dqn.epsilon_greedy(state, curr_player)\n",
    "            state, reward, done = env.step(action, verbose=True)\n",
    "            state = np.array([state])\n",
    "            num_steps += 1\n",
    "            if num_steps >= max_steps:\n",
    "                break\n",
    "        print(\"Game End\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820a13fd-5bfc-45da-afbb-4c6a9a5fd781",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_dqn(env, dqn, target_dqn, optimizer, replay_buffer, gamma=0.99, batch_size=32, num_episodes=1000, verbose=False):\n",
    "    \"\"\"\n",
    "    Trains the DQN model using experience replay.\n",
    "\n",
    "    Args:\n",
    "        env (SuperTicTacToeEnv): The game environment.\n",
    "        dqn (DQN): The DQN model to train.\n",
    "        target_dqn (DQN): The target DQN model for stable training.\n",
    "        optimizer (tf.keras.optimizers.Optimizer): The optimizer for training.\n",
    "        replay_buffer (ReplayBuffer): The replay buffer for storing experiences.\n",
    "        gamma (float): The discount factor for future rewards.\n",
    "        batch_size (int): The number of experiences to sample per training step.\n",
    "        num_episodes (int): The number of training episodes.\n",
    "        verbose (bool): Whether to print detailed training information.\n",
    "    \"\"\"\n",
    "    dqn(np.zeros((1, 5 * 4 * 4 + 1)))\n",
    "    target_dqn(np.zeros((1, 5 * 4 * 4 + 1)))\n",
    "    for episode in tqdm.trange(num_episodes):\n",
    "        state = env.reset()\n",
    "        state = np.array([state])\n",
    "        num_step = 0\n",
    "\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        episode_loss = []\n",
    "        while not done:\n",
    "            curr_player = env._current_player\n",
    "            action = dqn.epsilon_greedy(state, curr_player)\n",
    "\n",
    "            num_step += 1\n",
    "            next_state, reward, done = env.step(action, verbose)\n",
    "            next_state = np.array([next_state])\n",
    "\n",
    "            replay_buffer.push(state[0], action, reward, next_state[0], done, curr_player)\n",
    "            if verbose:\n",
    "                print(\"epoch {}\".format(episode), state[0].shape, action, reward, next_state[0].shape, done, curr_player)\n",
    "\n",
    "            if len(replay_buffer) > batch_size:\n",
    "                states, actions, rewards, next_states, dones, curr_players = replay_buffer.sample(batch_size)\n",
    "                states = np.array([s for s in states])\n",
    "                next_states = np.array([s for s in next_states])\n",
    "                curr_players_ = curr_players.reshape([batch_size, 1])\n",
    "\n",
    "                with tf.GradientTape() as tape:\n",
    "                    states_ = np.concatenate([states.flatten().reshape(batch_size, -1), curr_players_], axis=1)\n",
    "                    next_states_ = np.concatenate([next_states.flatten().reshape(batch_size, -1), curr_players_], axis=1)\n",
    "                    current_q_values = dqn(states_)\n",
    "                    next_q_values = target_dqn(next_states_)\n",
    "                    max_next_q_values = tf.reduce_max(next_q_values, axis=1)  # here to maximize?\n",
    "\n",
    "                    target_q_values = rewards + (1 - dones) * gamma * max_next_q_values\n",
    "                    target_q_values = tf.expand_dims(target_q_values, axis=1)\n",
    "\n",
    "                    indices = tf.stack([tf.range(batch_size), actions], axis=1)\n",
    "                    current_q_values = tf.gather_nd(current_q_values, indices)\n",
    "\n",
    "                    # Changed to tf.keras.losses.MSE\n",
    "                    loss = tf.keras.losses.MSE(target_q_values, current_q_values)\n",
    "                    episode_loss.append(loss)\n",
    "\n",
    "                gradients = tape.gradient(loss, dqn.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(gradients, dqn.trainable_variables))\n",
    "\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        # print('Episode {}: Total Reward = {}, num_steps {}, episode_avg_loss {}'.format(episode + 1, total_reward, num_step, np.mean(episode_loss)))\n",
    "\n",
    "        # update target_dqn periodically\n",
    "        if episode % 10 == 0:\n",
    "            target_dqn.set_weights(dqn.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0375ee3-d1ff-4db4-b90a-a9143f14c2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 500/500 [03:34<00:00,  2.33it/s]\n"
     ]
    }
   ],
   "source": [
    "env = SuperTicTacToeEnv()\n",
    "num_actions = 5 * 4 * 4\n",
    "dqn = DQN(num_actions)\n",
    "target_dqn = DQN(num_actions)\n",
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=1e-2)\n",
    "replay_buffer = ReplayBuffer(capacity=10000)\n",
    "\n",
    "train_dqn(env, dqn, target_dqn, optimizer, replay_buffer, batch_size=64, num_episodes=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3a45d35-3951-4358-bf66-ca7c125c1dfc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "player 1 action 18, board 1, row 0, col 2\n",
      "action 18 accepted directly\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "  0  0  1  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "player -1 action 77, board 4, row 3, col 1\n",
      "action 77 accepted directly\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "  0  0  1  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0 -1  0  0            \n",
      "player 1 action 78, board 4, row 3, col 2\n",
      "action 78 shift from (b4, r3 c2) to (b4, r4, c3), d=(1, 1)\n",
      "action 78 denied\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "  0  0  1  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0 -1  0  0            \n",
      "player -1 action 78, board 4, row 3, col 2\n",
      "action 78 accepted directly\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "  0  0  1  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0 -1 -1  0            \n",
      "player 1 action 30, board 1, row 3, col 2\n",
      "action 30 accepted directly\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "  0  0  1  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  1  0  0  0  0  0  0  0  0  0\n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0 -1 -1  0            \n",
      "player -1 action 34, board 2, row 0, col 2\n",
      "action 34 shift from (b2, r0 c2) to (b2, r0, c1), d=(0, -1)\n",
      "action 34 accepted\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "  0  0  1  0  0 -1  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  1  0  0  0  0  0  0  0  0  0\n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0 -1 -1  0            \n",
      "player 1 action 34, board 2, row 0, col 2\n",
      "action 34 accepted directly\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "  0  0  1  0  0 -1  1  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  1  0  0  0  0  0  0  0  0  0\n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0 -1 -1  0            \n",
      "player -1 action 21, board 1, row 1, col 1\n",
      "action 21 accepted directly\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "  0  0  1  0  0 -1  1  0  0  0  0  0\n",
      "  0 -1  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  1  0  0  0  0  0  0  0  0  0\n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0 -1 -1  0            \n",
      "player 1 action 7, board 0, row 1, col 3\n",
      "action 7 shift from (b0, r1 c3) to (b0, r0, c2), d=(-1, -1)\n",
      "action 7 accepted\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0  0  1  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "  0  0  1  0  0 -1  1  0  0  0  0  0\n",
      "  0 -1  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  1  0  0  0  0  0  0  0  0  0\n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0 -1 -1  0            \n",
      "player -1 action 7, board 0, row 1, col 3\n",
      "action 7 accepted directly\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0  0  1  0            \n",
      "              0  0  0 -1            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "  0  0  1  0  0 -1  1  0  0  0  0  0\n",
      "  0 -1  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  1  0  0  0  0  0  0  0  0  0\n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0 -1 -1  0            \n",
      "player 1 action 72, board 4, row 2, col 0\n",
      "action 72 shift from (b4, r2 c0) to (b4, r3, c0), d=(1, 0)\n",
      "action 72 accepted\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0  0  1  0            \n",
      "              0  0  0 -1            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "  0  0  1  0  0 -1  1  0  0  0  0  0\n",
      "  0 -1  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  1  0  0  0  0  0  0  0  0  0\n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "              1 -1 -1  0            \n",
      "player -1 action 72, board 4, row 2, col 0\n",
      "action 72 accepted directly\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0  0  1  0            \n",
      "              0  0  0 -1            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "  0  0  1  0  0 -1  1  0  0  0  0  0\n",
      "  0 -1  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  1  0  0  0  0  0  0  0  0  0\n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "             -1  0  0  0            \n",
      "              1 -1 -1  0            \n",
      "player 1 action 53, board 3, row 1, col 1\n",
      "action 53 accepted directly\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0  0  1  0            \n",
      "              0  0  0 -1            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "  0  0  1  0  0 -1  1  0  0  0  0  0\n",
      "  0 -1  0  0  0  0  0  0  0  1  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  1  0  0  0  0  0  0  0  0  0\n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "             -1  0  0  0            \n",
      "              1 -1 -1  0            \n",
      "player -1 action 3, board 0, row 0, col 3\n",
      "action 3 accepted directly\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0  0  1 -1            \n",
      "              0  0  0 -1            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "  0  0  1  0  0 -1  1  0  0  0  0  0\n",
      "  0 -1  0  0  0  0  0  0  0  1  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  1  0  0  0  0  0  0  0  0  0\n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "             -1  0  0  0            \n",
      "              1 -1 -1  0            \n",
      "player 1 action 50, board 3, row 0, col 2\n",
      "action 50 accepted directly\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0  0  1 -1            \n",
      "              0  0  0 -1            \n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "  0  0  1  0  0 -1  1  0  0  0  1  0\n",
      "  0 -1  0  0  0  0  0  0  0  1  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  1  0  0  0  0  0  0  0  0  0\n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "             -1  0  0  0            \n",
      "              1 -1 -1  0            \n",
      "player -1 action 15, board 0, row 3, col 3\n",
      "action 15 shift from (b0, r3 c3) to (b0, r2, c2), d=(-1, -1)\n",
      "action 15 accepted\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0  0  1 -1            \n",
      "              0  0  0 -1            \n",
      "              0  0 -1  0            \n",
      "              0  0  0  0            \n",
      "  0  0  1  0  0 -1  1  0  0  0  1  0\n",
      "  0 -1  0  0  0  0  0  0  0  1  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  1  0  0  0  0  0  0  0  0  0\n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "             -1  0  0  0            \n",
      "              1 -1 -1  0            \n",
      "player 1 action 15, board 0, row 3, col 3\n",
      "action 15 accepted directly\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0  0  1 -1            \n",
      "              0  0  0 -1            \n",
      "              0  0 -1  0            \n",
      "              0  0  0  1            \n",
      "  0  0  1  0  0 -1  1  0  0  0  1  0\n",
      "  0 -1  0  0  0  0  0  0  0  1  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  1  0  0  0  0  0  0  0  0  0\n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "             -1  0  0  0            \n",
      "              1 -1 -1  0            \n",
      "player -1 action 57, board 3, row 2, col 1\n",
      "action 57 accepted directly\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0  0  1 -1            \n",
      "              0  0  0 -1            \n",
      "              0  0 -1  0            \n",
      "              0  0  0  1            \n",
      "  0  0  1  0  0 -1  1  0  0  0  1  0\n",
      "  0 -1  0  0  0  0  0  0  0  1  0  0\n",
      "  0  0  0  0  0  0  0  0  0 -1  0  0\n",
      "  0  0  1  0  0  0  0  0  0  0  0  0\n",
      "              0  0  0  0            \n",
      "              0  0  0  0            \n",
      "             -1  0  0  0            \n",
      "              1 -1 -1  0            \n",
      "player 1 action 64, board 4, row 0, col 0\n",
      "action 64 accepted directly\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0  0  1 -1            \n",
      "              0  0  0 -1            \n",
      "              0  0 -1  0            \n",
      "              0  0  0  1            \n",
      "  0  0  1  0  0 -1  1  0  0  0  1  0\n",
      "  0 -1  0  0  0  0  0  0  0  1  0  0\n",
      "  0  0  0  0  0  0  0  0  0 -1  0  0\n",
      "  0  0  1  0  0  0  0  0  0  0  0  0\n",
      "              1  0  0  0            \n",
      "              0  0  0  0            \n",
      "             -1  0  0  0            \n",
      "              1 -1 -1  0            \n",
      "player -1 action 47, board 2, row 3, col 3\n",
      "action 47 shift from (b2, r3 c3) to (b2, r4, c4), d=(1, 1)\n",
      "action 47 denied\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0  0  1 -1            \n",
      "              0  0  0 -1            \n",
      "              0  0 -1  0            \n",
      "              0  0  0  1            \n",
      "  0  0  1  0  0 -1  1  0  0  0  1  0\n",
      "  0 -1  0  0  0  0  0  0  0  1  0  0\n",
      "  0  0  0  0  0  0  0  0  0 -1  0  0\n",
      "  0  0  1  0  0  0  0  0  0  0  0  0\n",
      "              1  0  0  0            \n",
      "              0  0  0  0            \n",
      "             -1  0  0  0            \n",
      "              1 -1 -1  0            \n",
      "player 1 action 47, board 2, row 3, col 3\n",
      "action 47 shift from (b2, r3 c3) to (b3, r2, c0), d=(-1, 1)\n",
      "action 47 accepted\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0  0  1 -1            \n",
      "              0  0  0 -1            \n",
      "              0  0 -1  0            \n",
      "              0  0  0  1            \n",
      "  0  0  1  0  0 -1  1  0  0  0  1  0\n",
      "  0 -1  0  0  0  0  0  0  0  1  0  0\n",
      "  0  0  0  0  0  0  0  0  1 -1  0  0\n",
      "  0  0  1  0  0  0  0  0  0  0  0  0\n",
      "              1  0  0  0            \n",
      "              0  0  0  0            \n",
      "             -1  0  0  0            \n",
      "              1 -1 -1  0            \n",
      "player -1 action 47, board 2, row 3, col 3\n",
      "action 47 accepted directly\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0  0  1 -1            \n",
      "              0  0  0 -1            \n",
      "              0  0 -1  0            \n",
      "              0  0  0  1            \n",
      "  0  0  1  0  0 -1  1  0  0  0  1  0\n",
      "  0 -1  0  0  0  0  0  0  0  1  0  0\n",
      "  0  0  0  0  0  0  0  0  1 -1  0  0\n",
      "  0  0  1  0  0  0  0 -1  0  0  0  0\n",
      "              1  0  0  0            \n",
      "              0  0  0  0            \n",
      "             -1  0  0  0            \n",
      "              1 -1 -1  0            \n",
      "player 1 action 38, board 2, row 1, col 2\n",
      "action 38 shift from (b2, r1 c2) to (b2, r2, c1), d=(1, -1)\n",
      "action 38 accepted\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0  0  1 -1            \n",
      "              0  0  0 -1            \n",
      "              0  0 -1  0            \n",
      "              0  0  0  1            \n",
      "  0  0  1  0  0 -1  1  0  0  0  1  0\n",
      "  0 -1  0  0  0  0  0  0  0  1  0  0\n",
      "  0  0  0  0  0  1  0  0  1 -1  0  0\n",
      "  0  0  1  0  0  0  0 -1  0  0  0  0\n",
      "              1  0  0  0            \n",
      "              0  0  0  0            \n",
      "             -1  0  0  0            \n",
      "              1 -1 -1  0            \n",
      "player -1 action 38, board 2, row 1, col 2\n",
      "action 38 accepted directly\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0  0  1 -1            \n",
      "              0  0  0 -1            \n",
      "              0  0 -1  0            \n",
      "              0  0  0  1            \n",
      "  0  0  1  0  0 -1  1  0  0  0  1  0\n",
      "  0 -1  0  0  0  0 -1  0  0  1  0  0\n",
      "  0  0  0  0  0  1  0  0  1 -1  0  0\n",
      "  0  0  1  0  0  0  0 -1  0  0  0  0\n",
      "              1  0  0  0            \n",
      "              0  0  0  0            \n",
      "             -1  0  0  0            \n",
      "              1 -1 -1  0            \n",
      "player 1 action 61, board 3, row 3, col 1\n",
      "action 61 accepted directly\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0  0  1 -1            \n",
      "              0  0  0 -1            \n",
      "              0  0 -1  0            \n",
      "              0  0  0  1            \n",
      "  0  0  1  0  0 -1  1  0  0  0  1  0\n",
      "  0 -1  0  0  0  0 -1  0  0  1  0  0\n",
      "  0  0  0  0  0  1  0  0  1 -1  0  0\n",
      "  0  0  1  0  0  0  0 -1  0  1  0  0\n",
      "              1  0  0  0            \n",
      "              0  0  0  0            \n",
      "             -1  0  0  0            \n",
      "              1 -1 -1  0            \n",
      "player -1 action 28, board 1, row 3, col 0\n",
      "action 28 shift from (b1, r3 c0) to (b1, r4, c-1), d=(1, -1)\n",
      "action 28 denied\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0  0  1 -1            \n",
      "              0  0  0 -1            \n",
      "              0  0 -1  0            \n",
      "              0  0  0  1            \n",
      "  0  0  1  0  0 -1  1  0  0  0  1  0\n",
      "  0 -1  0  0  0  0 -1  0  0  1  0  0\n",
      "  0  0  0  0  0  1  0  0  1 -1  0  0\n",
      "  0  0  1  0  0  0  0 -1  0  1  0  0\n",
      "              1  0  0  0            \n",
      "              0  0  0  0            \n",
      "             -1  0  0  0            \n",
      "              1 -1 -1  0            \n",
      "player 1 action 28, board 1, row 3, col 0\n",
      "action 28 accepted directly\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0  0  1 -1            \n",
      "              0  0  0 -1            \n",
      "              0  0 -1  0            \n",
      "              0  0  0  1            \n",
      "  0  0  1  0  0 -1  1  0  0  0  1  0\n",
      "  0 -1  0  0  0  0 -1  0  0  1  0  0\n",
      "  0  0  0  0  0  1  0  0  1 -1  0  0\n",
      "  1  0  1  0  0  0  0 -1  0  1  0  0\n",
      "              1  0  0  0            \n",
      "              0  0  0  0            \n",
      "             -1  0  0  0            \n",
      "              1 -1 -1  0            \n",
      "player -1 action 51, board 3, row 0, col 3\n",
      "action 51 accepted directly\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0  0  1 -1            \n",
      "              0  0  0 -1            \n",
      "              0  0 -1  0            \n",
      "              0  0  0  1            \n",
      "  0  0  1  0  0 -1  1  0  0  0  1 -1\n",
      "  0 -1  0  0  0  0 -1  0  0  1  0  0\n",
      "  0  0  0  0  0  1  0  0  1 -1  0  0\n",
      "  1  0  1  0  0  0  0 -1  0  1  0  0\n",
      "              1  0  0  0            \n",
      "              0  0  0  0            \n",
      "             -1  0  0  0            \n",
      "              1 -1 -1  0            \n",
      "player 1 action 23, board 1, row 1, col 3\n",
      "action 23 accepted directly\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0  0  1 -1            \n",
      "              0  0  0 -1            \n",
      "              0  0 -1  0            \n",
      "              0  0  0  1            \n",
      "  0  0  1  0  0 -1  1  0  0  0  1 -1\n",
      "  0 -1  0  1  0  0 -1  0  0  1  0  0\n",
      "  0  0  0  0  0  1  0  0  1 -1  0  0\n",
      "  1  0  1  0  0  0  0 -1  0  1  0  0\n",
      "              1  0  0  0            \n",
      "              0  0  0  0            \n",
      "             -1  0  0  0            \n",
      "              1 -1 -1  0            \n",
      "player -1 action 32, board 2, row 0, col 0\n",
      "action 32 shift from (b2, r0 c0) to (b2, r-1, c-1), d=(-1, -1)\n",
      "action 32 denied\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0  0  1 -1            \n",
      "              0  0  0 -1            \n",
      "              0  0 -1  0            \n",
      "              0  0  0  1            \n",
      "  0  0  1  0  0 -1  1  0  0  0  1 -1\n",
      "  0 -1  0  1  0  0 -1  0  0  1  0  0\n",
      "  0  0  0  0  0  1  0  0  1 -1  0  0\n",
      "  1  0  1  0  0  0  0 -1  0  1  0  0\n",
      "              1  0  0  0            \n",
      "              0  0  0  0            \n",
      "             -1  0  0  0            \n",
      "              1 -1 -1  0            \n",
      "player 1 action 30, board 1, row 3, col 2\n",
      "action 30 occupied already, reward -1.0\n",
      "reward -1.0, episode_ended False\n",
      "boards:\n",
      "              0  0  1 -1            \n",
      "              0  0  0 -1            \n",
      "              0  0 -1  0            \n",
      "              0  0  0  1            \n",
      "  0  0  1  0  0 -1  1  0  0  0  1 -1\n",
      "  0 -1  0  1  0  0 -1  0  0  1  0  0\n",
      "  0  0  0  0  0  1  0  0  1 -1  0  0\n",
      "  1  0  1  0  0  0  0 -1  0  1  0  0\n",
      "              1  0  0  0            \n",
      "              0  0  0  0            \n",
      "             -1  0  0  0            \n",
      "              1 -1 -1  0            \n",
      "player -1 action 32, board 2, row 0, col 0\n",
      "action 32 accepted directly\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0  0  1 -1            \n",
      "              0  0  0 -1            \n",
      "              0  0 -1  0            \n",
      "              0  0  0  1            \n",
      "  0  0  1  0 -1 -1  1  0  0  0  1 -1\n",
      "  0 -1  0  1  0  0 -1  0  0  1  0  0\n",
      "  0  0  0  0  0  1  0  0  1 -1  0  0\n",
      "  1  0  1  0  0  0  0 -1  0  1  0  0\n",
      "              1  0  0  0            \n",
      "              0  0  0  0            \n",
      "             -1  0  0  0            \n",
      "              1 -1 -1  0            \n",
      "player 1 action 79, board 4, row 3, col 3\n",
      "action 79 accepted directly\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0  0  1 -1            \n",
      "              0  0  0 -1            \n",
      "              0  0 -1  0            \n",
      "              0  0  0  1            \n",
      "  0  0  1  0 -1 -1  1  0  0  0  1 -1\n",
      "  0 -1  0  1  0  0 -1  0  0  1  0  0\n",
      "  0  0  0  0  0  1  0  0  1 -1  0  0\n",
      "  1  0  1  0  0  0  0 -1  0  1  0  0\n",
      "              1  0  0  0            \n",
      "              0  0  0  0            \n",
      "             -1  0  0  0            \n",
      "              1 -1 -1  1            \n",
      "player -1 action 20, board 1, row 1, col 0\n",
      "action 20 accepted directly\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0  0  1 -1            \n",
      "              0  0  0 -1            \n",
      "              0  0 -1  0            \n",
      "              0  0  0  1            \n",
      "  0  0  1  0 -1 -1  1  0  0  0  1 -1\n",
      " -1 -1  0  1  0  0 -1  0  0  1  0  0\n",
      "  0  0  0  0  0  1  0  0  1 -1  0  0\n",
      "  1  0  1  0  0  0  0 -1  0  1  0  0\n",
      "              1  0  0  0            \n",
      "              0  0  0  0            \n",
      "             -1  0  0  0            \n",
      "              1 -1 -1  1            \n",
      "player 1 action 45, board 2, row 3, col 1\n",
      "action 45 shift from (b2, r3 c1) to (b4, r0, c1), d=(1, 0)\n",
      "action 45 accepted\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0  0  1 -1            \n",
      "              0  0  0 -1            \n",
      "              0  0 -1  0            \n",
      "              0  0  0  1            \n",
      "  0  0  1  0 -1 -1  1  0  0  0  1 -1\n",
      " -1 -1  0  1  0  0 -1  0  0  1  0  0\n",
      "  0  0  0  0  0  1  0  0  1 -1  0  0\n",
      "  1  0  1  0  0  0  0 -1  0  1  0  0\n",
      "              1  1  0  0            \n",
      "              0  0  0  0            \n",
      "             -1  0  0  0            \n",
      "              1 -1 -1  1            \n",
      "player -1 action 45, board 2, row 3, col 1\n",
      "action 45 shift from (b2, r3 c1) to (b4, r0, c0), d=(1, -1)\n",
      "action 45 denied\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0  0  1 -1            \n",
      "              0  0  0 -1            \n",
      "              0  0 -1  0            \n",
      "              0  0  0  1            \n",
      "  0  0  1  0 -1 -1  1  0  0  0  1 -1\n",
      " -1 -1  0  1  0  0 -1  0  0  1  0  0\n",
      "  0  0  0  0  0  1  0  0  1 -1  0  0\n",
      "  1  0  1  0  0  0  0 -1  0  1  0  0\n",
      "              1  1  0  0            \n",
      "              0  0  0  0            \n",
      "             -1  0  0  0            \n",
      "              1 -1 -1  1            \n",
      "player 1 action 45, board 2, row 3, col 1\n",
      "action 45 shift from (b2, r3 c1) to (b4, r0, c2), d=(1, 1)\n",
      "action 45 accepted\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0  0  1 -1            \n",
      "              0  0  0 -1            \n",
      "              0  0 -1  0            \n",
      "              0  0  0  1            \n",
      "  0  0  1  0 -1 -1  1  0  0  0  1 -1\n",
      " -1 -1  0  1  0  0 -1  0  0  1  0  0\n",
      "  0  0  0  0  0  1  0  0  1 -1  0  0\n",
      "  1  0  1  0  0  0  0 -1  0  1  0  0\n",
      "              1  1  1  0            \n",
      "              0  0  0  0            \n",
      "             -1  0  0  0            \n",
      "              1 -1 -1  1            \n",
      "player -1 action 54, board 3, row 1, col 2\n",
      "action 54 shift from (b3, r1 c2) to (b3, r0, c1), d=(-1, -1)\n",
      "action 54 accepted\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0  0  1 -1            \n",
      "              0  0  0 -1            \n",
      "              0  0 -1  0            \n",
      "              0  0  0  1            \n",
      "  0  0  1  0 -1 -1  1  0  0 -1  1 -1\n",
      " -1 -1  0  1  0  0 -1  0  0  1  0  0\n",
      "  0  0  0  0  0  1  0  0  1 -1  0  0\n",
      "  1  0  1  0  0  0  0 -1  0  1  0  0\n",
      "              1  1  1  0            \n",
      "              0  0  0  0            \n",
      "             -1  0  0  0            \n",
      "              1 -1 -1  1            \n",
      "player 1 action 45, board 2, row 3, col 1\n",
      "action 45 accepted directly\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0  0  1 -1            \n",
      "              0  0  0 -1            \n",
      "              0  0 -1  0            \n",
      "              0  0  0  1            \n",
      "  0  0  1  0 -1 -1  1  0  0 -1  1 -1\n",
      " -1 -1  0  1  0  0 -1  0  0  1  0  0\n",
      "  0  0  0  0  0  1  0  0  1 -1  0  0\n",
      "  1  0  1  0  0  1  0 -1  0  1  0  0\n",
      "              1  1  1  0            \n",
      "              0  0  0  0            \n",
      "             -1  0  0  0            \n",
      "              1 -1 -1  1            \n",
      "player -1 action 59, board 3, row 2, col 3\n",
      "action 59 accepted directly\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0  0  1 -1            \n",
      "              0  0  0 -1            \n",
      "              0  0 -1  0            \n",
      "              0  0  0  1            \n",
      "  0  0  1  0 -1 -1  1  0  0 -1  1 -1\n",
      " -1 -1  0  1  0  0 -1  0  0  1  0  0\n",
      "  0  0  0  0  0  1  0  0  1 -1  0 -1\n",
      "  1  0  1  0  0  1  0 -1  0  1  0  0\n",
      "              1  1  1  0            \n",
      "              0  0  0  0            \n",
      "             -1  0  0  0            \n",
      "              1 -1 -1  1            \n",
      "player 1 action 9, board 0, row 2, col 1\n",
      "action 9 shift from (b0, r2 c1) to (b0, r3, c0), d=(1, -1)\n",
      "action 9 accepted\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0  0  1 -1            \n",
      "              0  0  0 -1            \n",
      "              0  0 -1  0            \n",
      "              1  0  0  1            \n",
      "  0  0  1  0 -1 -1  1  0  0 -1  1 -1\n",
      " -1 -1  0  1  0  0 -1  0  0  1  0  0\n",
      "  0  0  0  0  0  1  0  0  1 -1  0 -1\n",
      "  1  0  1  0  0  1  0 -1  0  1  0  0\n",
      "              1  1  1  0            \n",
      "              0  0  0  0            \n",
      "             -1  0  0  0            \n",
      "              1 -1 -1  1            \n",
      "player -1 action 9, board 0, row 2, col 1\n",
      "action 9 accepted directly\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0  0  1 -1            \n",
      "              0  0  0 -1            \n",
      "              0 -1 -1  0            \n",
      "              1  0  0  1            \n",
      "  0  0  1  0 -1 -1  1  0  0 -1  1 -1\n",
      " -1 -1  0  1  0  0 -1  0  0  1  0  0\n",
      "  0  0  0  0  0  1  0  0  1 -1  0 -1\n",
      "  1  0  1  0  0  1  0 -1  0  1  0  0\n",
      "              1  1  1  0            \n",
      "              0  0  0  0            \n",
      "             -1  0  0  0            \n",
      "              1 -1 -1  1            \n",
      "player 1 action 75, board 4, row 2, col 3\n",
      "action 75 accepted directly\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0  0  1 -1            \n",
      "              0  0  0 -1            \n",
      "              0 -1 -1  0            \n",
      "              1  0  0  1            \n",
      "  0  0  1  0 -1 -1  1  0  0 -1  1 -1\n",
      " -1 -1  0  1  0  0 -1  0  0  1  0  0\n",
      "  0  0  0  0  0  1  0  0  1 -1  0 -1\n",
      "  1  0  1  0  0  1  0 -1  0  1  0  0\n",
      "              1  1  1  0            \n",
      "              0  0  0  0            \n",
      "             -1  0  0  1            \n",
      "              1 -1 -1  1            \n",
      "player -1 action 48, board 3, row 0, col 0\n",
      "action 48 accepted directly\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0  0  1 -1            \n",
      "              0  0  0 -1            \n",
      "              0 -1 -1  0            \n",
      "              1  0  0  1            \n",
      "  0  0  1  0 -1 -1  1  0 -1 -1  1 -1\n",
      " -1 -1  0  1  0  0 -1  0  0  1  0  0\n",
      "  0  0  0  0  0  1  0  0  1 -1  0 -1\n",
      "  1  0  1  0  0  1  0 -1  0  1  0  0\n",
      "              1  1  1  0            \n",
      "              0  0  0  0            \n",
      "             -1  0  0  1            \n",
      "              1 -1 -1  1            \n",
      "player 1 action 1, board 0, row 0, col 1\n",
      "action 1 shift from (b0, r0 c1) to (b0, r1, c0), d=(1, -1)\n",
      "action 1 accepted\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0  0  1 -1            \n",
      "              1  0  0 -1            \n",
      "              0 -1 -1  0            \n",
      "              1  0  0  1            \n",
      "  0  0  1  0 -1 -1  1  0 -1 -1  1 -1\n",
      " -1 -1  0  1  0  0 -1  0  0  1  0  0\n",
      "  0  0  0  0  0  1  0  0  1 -1  0 -1\n",
      "  1  0  1  0  0  1  0 -1  0  1  0  0\n",
      "              1  1  1  0            \n",
      "              0  0  0  0            \n",
      "             -1  0  0  1            \n",
      "              1 -1 -1  1            \n",
      "player -1 action 1, board 0, row 0, col 1\n",
      "action 1 accepted directly\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0 -1  1 -1            \n",
      "              1  0  0 -1            \n",
      "              0 -1 -1  0            \n",
      "              1  0  0  1            \n",
      "  0  0  1  0 -1 -1  1  0 -1 -1  1 -1\n",
      " -1 -1  0  1  0  0 -1  0  0  1  0  0\n",
      "  0  0  0  0  0  1  0  0  1 -1  0 -1\n",
      "  1  0  1  0  0  1  0 -1  0  1  0  0\n",
      "              1  1  1  0            \n",
      "              0  0  0  0            \n",
      "             -1  0  0  1            \n",
      "              1 -1 -1  1            \n",
      "player 1 action 16, board 1, row 0, col 0\n",
      "action 16 accepted directly\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0 -1  1 -1            \n",
      "              1  0  0 -1            \n",
      "              0 -1 -1  0            \n",
      "              1  0  0  1            \n",
      "  1  0  1  0 -1 -1  1  0 -1 -1  1 -1\n",
      " -1 -1  0  1  0  0 -1  0  0  1  0  0\n",
      "  0  0  0  0  0  1  0  0  1 -1  0 -1\n",
      "  1  0  1  0  0  1  0 -1  0  1  0  0\n",
      "              1  1  1  0            \n",
      "              0  0  0  0            \n",
      "             -1  0  0  1            \n",
      "              1 -1 -1  1            \n",
      "player -1 action 63, board 3, row 3, col 3\n",
      "action 63 accepted directly\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0 -1  1 -1            \n",
      "              1  0  0 -1            \n",
      "              0 -1 -1  0            \n",
      "              1  0  0  1            \n",
      "  1  0  1  0 -1 -1  1  0 -1 -1  1 -1\n",
      " -1 -1  0  1  0  0 -1  0  0  1  0  0\n",
      "  0  0  0  0  0  1  0  0  1 -1  0 -1\n",
      "  1  0  1  0  0  1  0 -1  0  1  0 -1\n",
      "              1  1  1  0            \n",
      "              0  0  0  0            \n",
      "             -1  0  0  1            \n",
      "              1 -1 -1  1            \n",
      "player 1 action 11, board 0, row 2, col 3\n",
      "action 11 shift from (b0, r2 c3) to (b0, r2, c4), d=(0, 1)\n",
      "action 11 denied\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0 -1  1 -1            \n",
      "              1  0  0 -1            \n",
      "              0 -1 -1  0            \n",
      "              1  0  0  1            \n",
      "  1  0  1  0 -1 -1  1  0 -1 -1  1 -1\n",
      " -1 -1  0  1  0  0 -1  0  0  1  0  0\n",
      "  0  0  0  0  0  1  0  0  1 -1  0 -1\n",
      "  1  0  1  0  0  1  0 -1  0  1  0 -1\n",
      "              1  1  1  0            \n",
      "              0  0  0  0            \n",
      "             -1  0  0  1            \n",
      "              1 -1 -1  1            \n",
      "player -1 action 11, board 0, row 2, col 3\n",
      "action 11 accepted directly\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0 -1  1 -1            \n",
      "              1  0  0 -1            \n",
      "              0 -1 -1 -1            \n",
      "              1  0  0  1            \n",
      "  1  0  1  0 -1 -1  1  0 -1 -1  1 -1\n",
      " -1 -1  0  1  0  0 -1  0  0  1  0  0\n",
      "  0  0  0  0  0  1  0  0  1 -1  0 -1\n",
      "  1  0  1  0  0  1  0 -1  0  1  0 -1\n",
      "              1  1  1  0            \n",
      "              0  0  0  0            \n",
      "             -1  0  0  1            \n",
      "              1 -1 -1  1            \n",
      "player 1 action 47, board 2, row 3, col 3\n",
      "action 47 occupied already, reward -1.0\n",
      "reward -1.0, episode_ended False\n",
      "boards:\n",
      "              0 -1  1 -1            \n",
      "              1  0  0 -1            \n",
      "              0 -1 -1 -1            \n",
      "              1  0  0  1            \n",
      "  1  0  1  0 -1 -1  1  0 -1 -1  1 -1\n",
      " -1 -1  0  1  0  0 -1  0  0  1  0  0\n",
      "  0  0  0  0  0  1  0  0  1 -1  0 -1\n",
      "  1  0  1  0  0  1  0 -1  0  1  0 -1\n",
      "              1  1  1  0            \n",
      "              0  0  0  0            \n",
      "             -1  0  0  1            \n",
      "              1 -1 -1  1            \n",
      "player -1 action 26, board 1, row 2, col 2\n",
      "action 26 shift from (b1, r2 c2) to (b1, r1, c1), d=(-1, -1)\n",
      "action 26 denied\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0 -1  1 -1            \n",
      "              1  0  0 -1            \n",
      "              0 -1 -1 -1            \n",
      "              1  0  0  1            \n",
      "  1  0  1  0 -1 -1  1  0 -1 -1  1 -1\n",
      " -1 -1  0  1  0  0 -1  0  0  1  0  0\n",
      "  0  0  0  0  0  1  0  0  1 -1  0 -1\n",
      "  1  0  1  0  0  1  0 -1  0  1  0 -1\n",
      "              1  1  1  0            \n",
      "              0  0  0  0            \n",
      "             -1  0  0  1            \n",
      "              1 -1 -1  1            \n",
      "player 1 action 53, board 3, row 1, col 1\n",
      "action 53 occupied already, reward -1.0\n",
      "reward -1.0, episode_ended False\n",
      "boards:\n",
      "              0 -1  1 -1            \n",
      "              1  0  0 -1            \n",
      "              0 -1 -1 -1            \n",
      "              1  0  0  1            \n",
      "  1  0  1  0 -1 -1  1  0 -1 -1  1 -1\n",
      " -1 -1  0  1  0  0 -1  0  0  1  0  0\n",
      "  0  0  0  0  0  1  0  0  1 -1  0 -1\n",
      "  1  0  1  0  0  1  0 -1  0  1  0 -1\n",
      "              1  1  1  0            \n",
      "              0  0  0  0            \n",
      "             -1  0  0  1            \n",
      "              1 -1 -1  1            \n",
      "player -1 action 26, board 1, row 2, col 2\n",
      "action 26 accepted directly\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0 -1  1 -1            \n",
      "              1  0  0 -1            \n",
      "              0 -1 -1 -1            \n",
      "              1  0  0  1            \n",
      "  1  0  1  0 -1 -1  1  0 -1 -1  1 -1\n",
      " -1 -1  0  1  0  0 -1  0  0  1  0  0\n",
      "  0  0 -1  0  0  1  0  0  1 -1  0 -1\n",
      "  1  0  1  0  0  1  0 -1  0  1  0 -1\n",
      "              1  1  1  0            \n",
      "              0  0  0  0            \n",
      "             -1  0  0  1            \n",
      "              1 -1 -1  1            \n",
      "player 1 action 74, board 4, row 2, col 2\n",
      "action 74 shift from (b4, r2 c2) to (b4, r3, c1), d=(1, -1)\n",
      "action 74 denied\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0 -1  1 -1            \n",
      "              1  0  0 -1            \n",
      "              0 -1 -1 -1            \n",
      "              1  0  0  1            \n",
      "  1  0  1  0 -1 -1  1  0 -1 -1  1 -1\n",
      " -1 -1  0  1  0  0 -1  0  0  1  0  0\n",
      "  0  0 -1  0  0  1  0  0  1 -1  0 -1\n",
      "  1  0  1  0  0  1  0 -1  0  1  0 -1\n",
      "              1  1  1  0            \n",
      "              0  0  0  0            \n",
      "             -1  0  0  1            \n",
      "              1 -1 -1  1            \n",
      "player -1 action 74, board 4, row 2, col 2\n",
      "action 74 shift from (b4, r2 c2) to (b4, r1, c3), d=(-1, 1)\n",
      "action 74 accepted\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0 -1  1 -1            \n",
      "              1  0  0 -1            \n",
      "              0 -1 -1 -1            \n",
      "              1  0  0  1            \n",
      "  1  0  1  0 -1 -1  1  0 -1 -1  1 -1\n",
      " -1 -1  0  1  0  0 -1  0  0  1  0  0\n",
      "  0  0 -1  0  0  1  0  0  1 -1  0 -1\n",
      "  1  0  1  0  0  1  0 -1  0  1  0 -1\n",
      "              1  1  1  0            \n",
      "              0  0  0 -1            \n",
      "             -1  0  0  1            \n",
      "              1 -1 -1  1            \n",
      "player 1 action 74, board 4, row 2, col 2\n",
      "action 74 accepted directly\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0 -1  1 -1            \n",
      "              1  0  0 -1            \n",
      "              0 -1 -1 -1            \n",
      "              1  0  0  1            \n",
      "  1  0  1  0 -1 -1  1  0 -1 -1  1 -1\n",
      " -1 -1  0  1  0  0 -1  0  0  1  0  0\n",
      "  0  0 -1  0  0  1  0  0  1 -1  0 -1\n",
      "  1  0  1  0  0  1  0 -1  0  1  0 -1\n",
      "              1  1  1  0            \n",
      "              0  0  0 -1            \n",
      "             -1  0  1  1            \n",
      "              1 -1 -1  1            \n",
      "player -1 action 14, board 0, row 3, col 2\n",
      "action 14 accepted directly\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0 -1  1 -1            \n",
      "              1  0  0 -1            \n",
      "              0 -1 -1 -1            \n",
      "              1  0 -1  1            \n",
      "  1  0  1  0 -1 -1  1  0 -1 -1  1 -1\n",
      " -1 -1  0  1  0  0 -1  0  0  1  0  0\n",
      "  0  0 -1  0  0  1  0  0  1 -1  0 -1\n",
      "  1  0  1  0  0  1  0 -1  0  1  0 -1\n",
      "              1  1  1  0            \n",
      "              0  0  0 -1            \n",
      "             -1  0  1  1            \n",
      "              1 -1 -1  1            \n",
      "player 1 action 73, board 4, row 2, col 1\n",
      "action 73 accepted directly\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0 -1  1 -1            \n",
      "              1  0  0 -1            \n",
      "              0 -1 -1 -1            \n",
      "              1  0 -1  1            \n",
      "  1  0  1  0 -1 -1  1  0 -1 -1  1 -1\n",
      " -1 -1  0  1  0  0 -1  0  0  1  0  0\n",
      "  0  0 -1  0  0  1  0  0  1 -1  0 -1\n",
      "  1  0  1  0  0  1  0 -1  0  1  0 -1\n",
      "              1  1  1  0            \n",
      "              0  0  0 -1            \n",
      "             -1  1  1  1            \n",
      "              1 -1 -1  1            \n",
      "player -1 action 52, board 3, row 1, col 0\n",
      "action 52 accepted directly\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0 -1  1 -1            \n",
      "              1  0  0 -1            \n",
      "              0 -1 -1 -1            \n",
      "              1  0 -1  1            \n",
      "  1  0  1  0 -1 -1  1  0 -1 -1  1 -1\n",
      " -1 -1  0  1  0  0 -1  0 -1  1  0  0\n",
      "  0  0 -1  0  0  1  0  0  1 -1  0 -1\n",
      "  1  0  1  0  0  1  0 -1  0  1  0 -1\n",
      "              1  1  1  0            \n",
      "              0  0  0 -1            \n",
      "             -1  1  1  1            \n",
      "              1 -1 -1  1            \n",
      "player 1 action 46, board 2, row 3, col 2\n",
      "action 46 shift from (b2, r3 c2) to (b2, r2, c2), d=(-1, 0)\n",
      "action 46 accepted\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0 -1  1 -1            \n",
      "              1  0  0 -1            \n",
      "              0 -1 -1 -1            \n",
      "              1  0 -1  1            \n",
      "  1  0  1  0 -1 -1  1  0 -1 -1  1 -1\n",
      " -1 -1  0  1  0  0 -1  0 -1  1  0  0\n",
      "  0  0 -1  0  0  1  1  0  1 -1  0 -1\n",
      "  1  0  1  0  0  1  0 -1  0  1  0 -1\n",
      "              1  1  1  0            \n",
      "              0  0  0 -1            \n",
      "             -1  1  1  1            \n",
      "              1 -1 -1  1            \n",
      "player -1 action 46, board 2, row 3, col 2\n",
      "action 46 accepted directly\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0 -1  1 -1            \n",
      "              1  0  0 -1            \n",
      "              0 -1 -1 -1            \n",
      "              1  0 -1  1            \n",
      "  1  0  1  0 -1 -1  1  0 -1 -1  1 -1\n",
      " -1 -1  0  1  0  0 -1  0 -1  1  0  0\n",
      "  0  0 -1  0  0  1  1  0  1 -1  0 -1\n",
      "  1  0  1  0  0  1 -1 -1  0  1  0 -1\n",
      "              1  1  1  0            \n",
      "              0  0  0 -1            \n",
      "             -1  1  1  1            \n",
      "              1 -1 -1  1            \n",
      "player 1 action 44, board 2, row 3, col 0\n",
      "action 44 accepted directly\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0 -1  1 -1            \n",
      "              1  0  0 -1            \n",
      "              0 -1 -1 -1            \n",
      "              1  0 -1  1            \n",
      "  1  0  1  0 -1 -1  1  0 -1 -1  1 -1\n",
      " -1 -1  0  1  0  0 -1  0 -1  1  0  0\n",
      "  0  0 -1  0  0  1  1  0  1 -1  0 -1\n",
      "  1  0  1  0  1  1 -1 -1  0  1  0 -1\n",
      "              1  1  1  0            \n",
      "              0  0  0 -1            \n",
      "             -1  1  1  1            \n",
      "              1 -1 -1  1            \n",
      "player -1 action 67, board 4, row 0, col 3\n",
      "action 67 shift from (b4, r0 c3) to (b4, r1, c3), d=(1, 0)\n",
      "action 67 denied\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0 -1  1 -1            \n",
      "              1  0  0 -1            \n",
      "              0 -1 -1 -1            \n",
      "              1  0 -1  1            \n",
      "  1  0  1  0 -1 -1  1  0 -1 -1  1 -1\n",
      " -1 -1  0  1  0  0 -1  0 -1  1  0  0\n",
      "  0  0 -1  0  0  1  1  0  1 -1  0 -1\n",
      "  1  0  1  0  1  1 -1 -1  0  1  0 -1\n",
      "              1  1  1  0            \n",
      "              0  0  0 -1            \n",
      "             -1  1  1  1            \n",
      "              1 -1 -1  1            \n",
      "player 1 action 67, board 4, row 0, col 3\n",
      "action 67 shift from (b4, r0 c3) to (b4, r1, c4), d=(1, 1)\n",
      "action 67 denied\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0 -1  1 -1            \n",
      "              1  0  0 -1            \n",
      "              0 -1 -1 -1            \n",
      "              1  0 -1  1            \n",
      "  1  0  1  0 -1 -1  1  0 -1 -1  1 -1\n",
      " -1 -1  0  1  0  0 -1  0 -1  1  0  0\n",
      "  0  0 -1  0  0  1  1  0  1 -1  0 -1\n",
      "  1  0  1  0  1  1 -1 -1  0  1  0 -1\n",
      "              1  1  1  0            \n",
      "              0  0  0 -1            \n",
      "             -1  1  1  1            \n",
      "              1 -1 -1  1            \n",
      "player -1 action 67, board 4, row 0, col 3\n",
      "action 67 shift from (b4, r0 c3) to (b4, r1, c3), d=(1, 0)\n",
      "action 67 denied\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0 -1  1 -1            \n",
      "              1  0  0 -1            \n",
      "              0 -1 -1 -1            \n",
      "              1  0 -1  1            \n",
      "  1  0  1  0 -1 -1  1  0 -1 -1  1 -1\n",
      " -1 -1  0  1  0  0 -1  0 -1  1  0  0\n",
      "  0  0 -1  0  0  1  1  0  1 -1  0 -1\n",
      "  1  0  1  0  1  1 -1 -1  0  1  0 -1\n",
      "              1  1  1  0            \n",
      "              0  0  0 -1            \n",
      "             -1  1  1  1            \n",
      "              1 -1 -1  1            \n",
      "player 1 action 67, board 4, row 0, col 3\n",
      "action 67 shift from (b4, r0 c3) to (b4, r0, c4), d=(0, 1)\n",
      "action 67 denied\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0 -1  1 -1            \n",
      "              1  0  0 -1            \n",
      "              0 -1 -1 -1            \n",
      "              1  0 -1  1            \n",
      "  1  0  1  0 -1 -1  1  0 -1 -1  1 -1\n",
      " -1 -1  0  1  0  0 -1  0 -1  1  0  0\n",
      "  0  0 -1  0  0  1  1  0  1 -1  0 -1\n",
      "  1  0  1  0  1  1 -1 -1  0  1  0 -1\n",
      "              1  1  1  0            \n",
      "              0  0  0 -1            \n",
      "             -1  1  1  1            \n",
      "              1 -1 -1  1            \n",
      "player -1 action 67, board 4, row 0, col 3\n",
      "action 67 accepted directly\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0 -1  1 -1            \n",
      "              1  0  0 -1            \n",
      "              0 -1 -1 -1            \n",
      "              1  0 -1  1            \n",
      "  1  0  1  0 -1 -1  1  0 -1 -1  1 -1\n",
      " -1 -1  0  1  0  0 -1  0 -1  1  0  0\n",
      "  0  0 -1  0  0  1  1  0  1 -1  0 -1\n",
      "  1  0  1  0  1  1 -1 -1  0  1  0 -1\n",
      "              1  1  1 -1            \n",
      "              0  0  0 -1            \n",
      "             -1  1  1  1            \n",
      "              1 -1 -1  1            \n",
      "player 1 action 13, board 0, row 3, col 1\n",
      "action 13 accepted directly\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0 -1  1 -1            \n",
      "              1  0  0 -1            \n",
      "              0 -1 -1 -1            \n",
      "              1  1 -1  1            \n",
      "  1  0  1  0 -1 -1  1  0 -1 -1  1 -1\n",
      " -1 -1  0  1  0  0 -1  0 -1  1  0  0\n",
      "  0  0 -1  0  0  1  1  0  1 -1  0 -1\n",
      "  1  0  1  0  1  1 -1 -1  0  1  0 -1\n",
      "              1  1  1 -1            \n",
      "              0  0  0 -1            \n",
      "             -1  1  1  1            \n",
      "              1 -1 -1  1            \n",
      "player -1 action 24, board 1, row 2, col 0\n",
      "action 24 accepted directly\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0 -1  1 -1            \n",
      "              1  0  0 -1            \n",
      "              0 -1 -1 -1            \n",
      "              1  1 -1  1            \n",
      "  1  0  1  0 -1 -1  1  0 -1 -1  1 -1\n",
      " -1 -1  0  1  0  0 -1  0 -1  1  0  0\n",
      " -1  0 -1  0  0  1  1  0  1 -1  0 -1\n",
      "  1  0  1  0  1  1 -1 -1  0  1  0 -1\n",
      "              1  1  1 -1            \n",
      "              0  0  0 -1            \n",
      "             -1  1  1  1            \n",
      "              1 -1 -1  1            \n",
      "player 1 action 25, board 1, row 2, col 1\n",
      "action 25 shift from (b1, r2 c1) to (b1, r3, c0), d=(1, -1)\n",
      "action 25 denied\n",
      "reward 0, episode_ended False\n",
      "boards:\n",
      "              0 -1  1 -1            \n",
      "              1  0  0 -1            \n",
      "              0 -1 -1 -1            \n",
      "              1  1 -1  1            \n",
      "  1  0  1  0 -1 -1  1  0 -1 -1  1 -1\n",
      " -1 -1  0  1  0  0 -1  0 -1  1  0  0\n",
      " -1  0 -1  0  0  1  1  0  1 -1  0 -1\n",
      "  1  0  1  0  1  1 -1 -1  0  1  0 -1\n",
      "              1  1  1 -1            \n",
      "              0  0  0 -1            \n",
      "             -1  1  1  1            \n",
      "              1 -1 -1  1            \n",
      "player -1 action 34, board 2, row 0, col 2\n",
      "action 34 occupied already, reward -1.0\n",
      "reward -1.0, episode_ended False\n",
      "boards:\n",
      "              0 -1  1 -1            \n",
      "              1  0  0 -1            \n",
      "              0 -1 -1 -1            \n",
      "              1  1 -1  1            \n",
      "  1  0  1  0 -1 -1  1  0 -1 -1  1 -1\n",
      " -1 -1  0  1  0  0 -1  0 -1  1  0  0\n",
      " -1  0 -1  0  0  1  1  0  1 -1  0 -1\n",
      "  1  0  1  0  1  1 -1 -1  0  1  0 -1\n",
      "              1  1  1 -1            \n",
      "              0  0  0 -1            \n",
      "             -1  1  1  1            \n",
      "              1 -1 -1  1            \n",
      "player 1 action 27, board 1, row 2, col 3\n",
      "action 27 shift from (b1, r2 c3) to (b2, r2, c0), d=(0, 1)\n",
      "action 27 accepted\n",
      "reward 1.0, episode_ended True\n",
      "boards:\n",
      "              0 -1  1 -1            \n",
      "              1  0  0 -1            \n",
      "              0 -1 -1 -1            \n",
      "              1  1 -1  1            \n",
      "  1  0  1  0 -1 -1  1  0 -1 -1  1 -1\n",
      " -1 -1  0  1  0  0 -1  0 -1  1  0  0\n",
      " -1  0 -1  0  1  1  1  0  1 -1  0 -1\n",
      "  1  0  1  0  1  1 -1 -1  0  1  0 -1\n",
      "              1  1  1 -1            \n",
      "              0  0  0 -1            \n",
      "             -1  1  1  1            \n",
      "              1 -1 -1  1            \n",
      "Game End\n"
     ]
    }
   ],
   "source": [
    "test_dqn(env, dqn, max_steps=500, num_games=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78819eee-f702-48e5-af08-b6a802fa4e5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
